{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8784285,"sourceType":"datasetVersion","datasetId":5280683}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Training a Regularized Logistic Regression model for Machine Failure Prediction using NumPy","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport math\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nimport pickle as pkl","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.005024Z","iopub.execute_input":"2024-07-14T08:47:30.005499Z","iopub.status.idle":"2024-07-14T08:47:30.012354Z","shell.execute_reply.started":"2024-07-14T08:47:30.005468Z","shell.execute_reply":"2024-07-14T08:47:30.011183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring the Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/machine-failure-prediction-using-sensor-data/data.csv') # importing the data\nload = False\n\n# Optional: load the trained parameters\n'''\nwith open('trained_parameters.pkl', 'rb') as file:\n    loaded_parameters = pkl.load(file)\n\n    # Extract parameters and hyperparameters\n    w = loaded_parameters['weights']\n    b = loaded_parameters['bias']\n    epochs =  loaded_parameters['epochs']\n    alpha = loaded_parameters['learning rate']\n    lambda_reg = loaded_parameters['reg parameter']\n    \n    load = True\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.014572Z","iopub.execute_input":"2024-07-14T08:47:30.015233Z","iopub.status.idle":"2024-07-14T08:47:30.032046Z","shell.execute_reply.started":"2024-07-14T08:47:30.015202Z","shell.execute_reply":"2024-07-14T08:47:30.030944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe() # getting the summary of the data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.033232Z","iopub.execute_input":"2024-07-14T08:47:30.033534Z","iopub.status.idle":"2024-07-14T08:47:30.070193Z","shell.execute_reply.started":"2024-07-14T08:47:30.033503Z","shell.execute_reply":"2024-07-14T08:47:30.069067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info() # getting the information of the data regarding the data types of features, number of non-null values, etc","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.071393Z","iopub.execute_input":"2024-07-14T08:47:30.071694Z","iopub.status.idle":"2024-07-14T08:47:30.082157Z","shell.execute_reply.started":"2024-07-14T08:47:30.071669Z","shell.execute_reply":"2024-07-14T08:47:30.080932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.duplicated().sum() # checking for the number of duplicate rows in the data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.085542Z","iopub.execute_input":"2024-07-14T08:47:30.086044Z","iopub.status.idle":"2024-07-14T08:47:30.098222Z","shell.execute_reply.started":"2024-07-14T08:47:30.086007Z","shell.execute_reply":"2024-07-14T08:47:30.096755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head() # getting the first 5 rows of the data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.099796Z","iopub.execute_input":"2024-07-14T08:47:30.100322Z","iopub.status.idle":"2024-07-14T08:47:30.115885Z","shell.execute_reply.started":"2024-07-14T08:47:30.100265Z","shell.execute_reply":"2024-07-14T08:47:30.114671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My initial impressions of the dataset:\n\n* There are 944 total rows, with 1 duplicated row.\n* All data is in numeric form, so there isn't much data pre-processing needed.\n* There are no missing values.","metadata":{}},{"cell_type":"code","source":"df.drop_duplicates(inplace=True) # dropping the duplicate rows","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.117508Z","iopub.execute_input":"2024-07-14T08:47:30.117939Z","iopub.status.idle":"2024-07-14T08:47:30.127142Z","shell.execute_reply.started":"2024-07-14T08:47:30.117901Z","shell.execute_reply":"2024-07-14T08:47:30.126118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking Skewness of Dataset","metadata":{}},{"cell_type":"markdown","source":"Checking if the dataset is imbalanced and skewed in any way","metadata":{}},{"cell_type":"code","source":"color = ['#92B6B1', '#43C59E'] # defining the colors for the plots\nsns.set_style('darkgrid') # setting the style for the plot\ndf['fail'].value_counts().plot.bar(color=color) # plotting the bar plot for the target variable\nplt.xlabel('Fail') # setting the x-axis label\nplt.ylabel('Total') # setting the y-axis label\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.128407Z","iopub.execute_input":"2024-07-14T08:47:30.128848Z","iopub.status.idle":"2024-07-14T08:47:30.346859Z","shell.execute_reply.started":"2024-07-14T08:47:30.128816Z","shell.execute_reply":"2024-07-14T08:47:30.345721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like the data is skewed, as the count of positive examples are lower than the negative examples. I will use SMOTE, an over sampling technique to increase the count of positive examples to make the distribution a little better.","metadata":{}},{"cell_type":"markdown","source":"### Over sampling to decrease Skewness","metadata":{}},{"cell_type":"code","source":"over_sampler = SMOTE(random_state=42) # initializing the SMOTE object\nX = df.drop(['fail'], axis=1) # seperating the features from the target feature\ny = df['fail'] # isolating the target feature\nX, y = over_sampler.fit_resample(X, y) # resampling the dataset","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.348109Z","iopub.execute_input":"2024-07-14T08:47:30.348417Z","iopub.status.idle":"2024-07-14T08:47:30.364925Z","shell.execute_reply.started":"2024-07-14T08:47:30.348393Z","shell.execute_reply":"2024-07-14T08:47:30.363748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts().plot.bar(color=color) # plotting the bar plot for the target variable after resampling","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.366476Z","iopub.execute_input":"2024-07-14T08:47:30.366828Z","iopub.status.idle":"2024-07-14T08:47:30.558889Z","shell.execute_reply.started":"2024-07-14T08:47:30.366799Z","shell.execute_reply":"2024-07-14T08:47:30.557738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the count of positive and negative examples is much better. Now I will scale the dataset and then train a model.","metadata":{}},{"cell_type":"markdown","source":"### Feature Scaling","metadata":{}},{"cell_type":"markdown","source":"I will use a min-max scaler, it will scale the values of features to a value between the range 0 and 1.","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler() # Initializing the MinMaxScaler\nX_scaled = scaler.fit_transform(X) # scaling the data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.562712Z","iopub.execute_input":"2024-07-14T08:47:30.563219Z","iopub.status.idle":"2024-07-14T08:47:30.572226Z","shell.execute_reply.started":"2024-07-14T08:47:30.563181Z","shell.execute_reply":"2024-07-14T08:47:30.571156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Splitting","metadata":{}},{"cell_type":"markdown","source":"Next, we will split the data into Test and Train sets. This is an important step. You want a way to evaluate the model, and splitting the data into Train and Test sets gives you that option. Usually, you also would want to split the Test set further into Cross-validation and Test sets, and use the Cross-validation set for hyperparameter tuning. I used that approach to tune the hyperparameters, and now that I have finalized the values for hyperparameters, I will skip splitting the data further into cross-validation set and test set, and just directly split the data into Training and Test sets. I will do a 70:30 split on the dataset.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # splitting the data into training and testing sets","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.573644Z","iopub.execute_input":"2024-07-14T08:47:30.574096Z","iopub.status.idle":"2024-07-14T08:47:30.583769Z","shell.execute_reply.started":"2024-07-14T08:47:30.574061Z","shell.execute_reply":"2024-07-14T08:47:30.582645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What is Regularization?","metadata":{}},{"cell_type":"markdown","source":"To understand regularization, fist let's understand what is overfitting. Overfitting is an undesirable behavior in Machine Learning in which a ML algorithm doesn't generalize well to all types of data that can occur, and fits too closely to the training data. In other words, it gives too much emphasis on the training data, so much so that if it encounters any other type of data that is a little different to the data it \"learned\", it is unable to give a satisfactory prediction. This phenomena can also be stated as the model having \"High Variance\".\n\nThis is where regularization comes in. Regularization prevents ovefitting by adding a penalty term in the cost function, which discourages the model to give to much importance to the training data. There are three types of regularization, but I have used the L2 Regularization, also called Ridge regularization. In this type of regularization, we add the sqaured sum of the magnitudes of the weights in the cost function. Regularization also adds a hyperparameter called lambda, which is a regularization parameter which basically affects the effect the penalty term makes on the overall cost function.","metadata":{}},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"markdown","source":"I will be training a regularized logistic regression model. I will implement everything from scratch, using just numpy and maths. I will be using the Binary Cross Entropy cost function, and I will be optimizing it using Gradient Descent. The functions for gradient descent and others are defined below.","metadata":{}},{"cell_type":"code","source":"def compute_cost(w, b, X, y, lambda_reg):\n    '''\n    Compute the cost function for logistic regression\n    \n    Parameters:\n        w (numpy array): weight parameters of the model\n        b (float): bias parameter of the model\n        X (numpy array): features of the dataset excluding the target feature\n        y (numpy array): target feature of the dataset\n        lambda_reg (int): regularization parameter\n    \n    Returns:\n        cost (float): the cost of the model with respect to the current weights and bias\n    '''\n    m = X.shape[0] # getting the number of examples in the dataset\n    z = np.dot(X, w) + b # computing the linear combination of the weights and features\n    f_wb = 1/(np.exp(-z) + 1) # applying the sigmoid function to the linear combination\n    cost = (-1/m) * np.sum(y * np.log(f_wb) + (1-y) * np.log(1-f_wb)) + (lambda_reg/(2*m) * np.sum(w**2)) # computing the cost function\n    return cost\n\n\ndef gradient_descent(w, b, X, y, lambda_reg):\n    '''\n    Compute the derivatives of the cost function with respect to the weights and bias.\n    \n    Parameters:\n        w (numpy array): weight parameters of the model\n        b (float): bias parameter of the model\n        X (numpy array): features of the dataset excluding the target feature\n        y (numpy array): target feature of the dataset\n        lambda_reg (int): regularization parameter\n    \n    Returns:\n        dj_dw (numpy array): the value of the derivative of the cost function with respect to the weights\n        dj_db (float): the value of the derivative of the cost function with respect to the bias\n    '''\n    m, n = X.shape # getting the number of examples and features in the dataset\n    z = np.dot(X, w) + b # computing the linear combination of the weights and features\n    f_wb = 1/(np.exp(-z) + 1) # applying the sigmoid function to the linear combination\n    err = f_wb - y  # computing the error term\n    dj_dw = (1/m) * np.dot(X.T, err) + (lambda_reg/m) * w # computing the derivative of the cost function with respect to the weights, and adding the regularization term\n    dj_db = (1/m) * np.sum(err) # computing the derivative of the cost function with respect to the bias. Since I am only regularizing the weights, I will skip adding the regularization term to the bias. Although, that is an option that you can implement\n    return dj_dw, dj_db\n\n\ndef compute_GD(w, b, X, y, epochs, alpha, lambda_reg, stopping_criteria=0.00001):\n    '''\n    Implemented gradient descent algorithm to find the optimal weights and bias for the logistic regression model.\n\n    Parameters:\n        w (numpy array): weight parameters of the model\n        b (float): bias parameter of the model\n        X (numpy array): features of the dataset excluding the target feature\n        y (numpy array): target feature of the dataset\n        epochs (int): number of iterations to run the gradient descent algorithm\n        alpha (float): learning rate\n        lambda_reg (int): regularization parameter\n        stopping_criteria (float): the difference between the cost of the current iteration and the previous iteration to stop the algorithm\n\n    Returns:\n        w (numpy array): the optimal weight parameters of the model\n        b (float): the optimal bias parameter of the model\n        JD_history (list): the cost of the model at each iteration for visualization purposes\n    '''\n    JD_history = [] # initializing the list to store the cost of the model at each iteration, for visualization purposes\n    \n    for i in range(epochs): # looping through the number of iterations\n        dj_dw, dj_db = gradient_descent(w, b, X, y, lambda_reg) # computing the derivatives of the cost function with respect to the weights and bias\n        w = w - alpha * dj_dw # updating the weights\n        b = b - alpha * dj_db # updating the bias\n        \n        if i<100000: # storing the cost of the model at each iteration\n            JD_history.append(compute_cost(w, b, X, y, lambda_reg)) # computing the cost of the model at each iteration\n        \n        if i% math.ceil(epochs/10) == 0: # printing the cost of the model at every 100th iterations\n            print(\"Epoch:{:4}, Loss:{:4.2f}\".format(i, JD_history[-1])) # printing the cost and iteration number\n        \n        if i>1 and abs(JD_history[-1] - JD_history[-2]) < stopping_criteria: # checking if the difference between the cost of the current iteration and the previous iteration is less than the stopping criteria\n            break\n    \n    return w, b, JD_history","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.585267Z","iopub.execute_input":"2024-07-14T08:47:30.586070Z","iopub.status.idle":"2024-07-14T08:47:30.602056Z","shell.execute_reply.started":"2024-07-14T08:47:30.586030Z","shell.execute_reply":"2024-07-14T08:47:30.601003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not load:\n    # Defining the hyperparameters\n\n    epochs = 10000 # number of iterations\n    alpha = 0.01 # learning rate\n    lambda_reg = 1 # regularization parameter\n\n    w = np.zeros(X_train.shape[1]) # initializing the weights\n    b = 0 # initializing the bias\n\n    w, b, JD_history = compute_GD(w, b, X_train, y_train, epochs, alpha, lambda_reg) # training the model","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:30.603343Z","iopub.execute_input":"2024-07-14T08:47:30.603769Z","iopub.status.idle":"2024-07-14T08:47:35.710974Z","shell.execute_reply.started":"2024-07-14T08:47:30.603735Z","shell.execute_reply":"2024-07-14T08:47:35.709929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(JD_history) # plotting the cost of the model at each iteration\nplt.xlabel('Iterations') # setting the x-axis label\nplt.ylabel('Loss') # setting the y-axis label","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:35.712117Z","iopub.execute_input":"2024-07-14T08:47:35.712406Z","iopub.status.idle":"2024-07-14T08:47:36.072202Z","shell.execute_reply.started":"2024-07-14T08:47:35.712383Z","shell.execute_reply":"2024-07-14T08:47:36.071101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The loss seems to be decreasing at a decent pace, there are no signs that we are shooting over the global optimum (the loss is constantly decreasing), which would have meant our learning rate is too high.","metadata":{}},{"cell_type":"markdown","source":"### Evaluating Model","metadata":{}},{"cell_type":"markdown","source":"I will be evaluating the model using classical classification metrics like accuracy, precision, recall, etc. I will also generate a classification report and a confusion matrix so that we know the performance of the model for each class as well. I am setting the threshold of the model to 0.5. We are using a threshold because when we apply the sigmoid function to the output to the linear combination of weights and bias, it generates a value between 0 and 1, which is basically a probability of the example being of class 1. By applying a threshold, we are setting the criteria for the probability that an example should have to be labelled as a positive example (or belonging to class 1). If the probability of an example is 0.5 or above, it is labelled as 1. Anything otherwise is labelled 0.","metadata":{}},{"cell_type":"code","source":"y_test_pred = [] # initializing the list to store the predictions of the model\nthreshold = 0.5 # setting the threshold for the model\nm = X_test.shape[0] # getting the number of examples in the test set\nfor i in range(m):\n    z = np.dot(X_test[i], w) + b # computing the linear combination of the weights and features\n    f_wb = 1/(np.exp(-z) + 1) # applying the sigmoid function to the linear combination\n    y_test_pred.append(1 if f_wb >= threshold else 0) # making the prediction based on the threshold","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:36.073744Z","iopub.execute_input":"2024-07-14T08:47:36.074173Z","iopub.status.idle":"2024-07-14T08:47:36.083922Z","shell.execute_reply.started":"2024-07-14T08:47:36.074138Z","shell.execute_reply":"2024-07-14T08:47:36.082741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix","metadata":{}},{"cell_type":"code","source":"conf_matrix = confusion_matrix(y_test, y_test_pred) # computing the confusion matrix\ndisp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix) # initializing the confusion matrix display\ndisp.plot() # plotting the confusion matrix","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:54.449260Z","iopub.execute_input":"2024-07-14T08:47:54.449670Z","iopub.status.idle":"2024-07-14T08:47:54.804882Z","shell.execute_reply.started":"2024-07-14T08:47:54.449615Z","shell.execute_reply":"2024-07-14T08:47:54.803572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Classification Report","metadata":{}},{"cell_type":"code","source":"class_report = classification_report(y_test, y_test_pred) # computing the classification report\nprint(\"Classification Report: \\n\", class_report) # printing the classification report","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:48:00.310649Z","iopub.execute_input":"2024-07-14T08:48:00.311972Z","iopub.status.idle":"2024-07-14T08:48:00.328001Z","shell.execute_reply.started":"2024-07-14T08:48:00.311925Z","shell.execute_reply":"2024-07-14T08:48:00.326439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_pred = np.array(y_test_pred) # converting the predictions to a numpy array\naccuracy = accuracy_score(y_test, y_test_pred) # computing the accuracy of the model\nprecision = precision_score(y_test, y_test_pred) # computing the precision of the model\nrecall = recall_score(y_test, y_test_pred) # computing the recall of the model\nf1 = f1_score(y_test, y_test_pred) # computing the f1 score of the model","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:36.395680Z","iopub.execute_input":"2024-07-14T08:47:36.396077Z","iopub.status.idle":"2024-07-14T08:47:36.410744Z","shell.execute_reply.started":"2024-07-14T08:47:36.396047Z","shell.execute_reply":"2024-07-14T08:47:36.409521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = {'Accuracy': accuracy, 'Precision': precision, 'Recall': recall, 'F1 Score': f1} # storing the scores in a dictionary\n\ncolor = ['#92B6B1', '#43C59E', '#2F4858', '#33658A'] # defining the colors for the plot\nsns.set_style('darkgrid') # setting the style for the plot\nplt.figure(figsize=(10, 6)) # setting the figure size\nplt.bar(scores.keys(), scores.values(), color=color) # plotting the bar plot for the scores\nplt.ylabel('Score') # setting the y-axis label\nplt.ylim(0, 1)  # setting the range of values to be shown on the y-axis, which is between 0 and 1\nplt.title('Evaluation Scores') # setting the title of the plot\nplt.show() # displaying the plot","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:36.412279Z","iopub.execute_input":"2024-07-14T08:47:36.412710Z","iopub.status.idle":"2024-07-14T08:47:36.682642Z","shell.execute_reply.started":"2024-07-14T08:47:36.412670Z","shell.execute_reply":"2024-07-14T08:47:36.681497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the trained parameters\nif not load:\n    parameters = {\n        'weights': w,\n        'bias': b,\n        'epochs': epochs,\n        'learning rate': alpha,\n        'reg parameter': lambda_reg\n    }\n\n    with open('trained_parameters.pkl', 'wb') as file:\n        pkl.dump(parameters, file)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:47:36.684071Z","iopub.execute_input":"2024-07-14T08:47:36.684451Z","iopub.status.idle":"2024-07-14T08:47:36.691679Z","shell.execute_reply.started":"2024-07-14T08:47:36.684415Z","shell.execute_reply":"2024-07-14T08:47:36.690633Z"},"trusted":true},"execution_count":null,"outputs":[]}]}